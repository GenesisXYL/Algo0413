{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir=\"./data/\"\n",
    "ticker=\"TSLA\"\n",
    "\n",
    "date_pool=pd.date_range(\"1/1/2019\",\"1/31/2019\",freq=\"B\").strftime(\"%Y%m%d\")\n",
    "date_pool=[d for d in date_pool if os.path.exists(data_dir+\"trades_{}_{}.csv\".format(d,ticker))]\n",
    "\n",
    "train_days=10\n",
    "train_date_list=date_pool[:train_days]\n",
    "test_date_list=date_pool[train_days+1:]\n",
    "time_steps = 50\n",
    "\n",
    "nforward=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline():\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.scaler = None\n",
    "    def load_data(self, ticker, date):\n",
    "        df = pd.read_csv(data_dir+'trades_{}_{}.csv'.format(date, ticker),index_col=[0],parse_dates=[0])\n",
    "\n",
    "        # Feature Engineering\n",
    "        df[\"direction\"]=(df[\"trade_px\"]-df[\"trade_px\"].shift(1)).apply(np.sign)\n",
    "        df[\"pct_change\"]=df[\"trade_px\"].pct_change()\n",
    "\n",
    "        mysign=lambda x: 0 if abs(x)<1e-5 else (1 if x>0 else -1)\n",
    "        df[\"label\"]=(df[\"trade_px\"].rolling(nforward).mean().shift(-nforward)-df[\"trade_px\"]).apply(mysign)\n",
    "        # df[\"label\"]=(df[\"trade_px\"].shift(-1)-df[\"trade_px\"]).apply(np.sign) # last version\n",
    "\n",
    "        df.fillna(method=\"ffill\",inplace=True)\n",
    "        df.dropna(axis=0,inplace=True)\n",
    "        # print(df.head(10),df.shape)\n",
    "        # print(\"NaN number: \",df.isna().sum().sum())\n",
    "\n",
    "        return df[[\"trade_px\",\"trade_size\",\"pct_change\",\"direction\",\"label\"]].values\n",
    "\n",
    "    def create_dataset(self, ticker=ticker, dates=train_date_list, time_steps = time_steps, input_scaler=None):  \n",
    "        for i,d in enumerate(dates):\n",
    "            datanew = self.load_data(ticker,d)\n",
    "            if i==0:\n",
    "                data=datanew\n",
    "            else:\n",
    "                data=np.vstack((data, datanew))\n",
    "\n",
    "        label=data[:,-1]\n",
    "        data=data[:,:-1]\n",
    "\n",
    "        if input_scaler is None:\n",
    "            scaler=StandardScaler()\n",
    "            data=scaler.fit_transform(data)\n",
    "        else:\n",
    "            data=input_scaler.transform(data)\n",
    "            scaler=input_scaler\n",
    "\n",
    "        x = [data[0 : time_steps]]\n",
    "        y = [label[time_steps-1]]\n",
    "        N=len(data)//time_steps\n",
    "\n",
    "        print(N)\n",
    "        for i in range(1, N):\n",
    "            t = data[i*time_steps: (i + 1)*time_steps]\n",
    "            x = np.vstack((x, [t]))\n",
    "            y.append(label[(i + 1)*time_steps-1])\n",
    "\n",
    "        y=pd.get_dummies(y)\n",
    "        #print(y)\n",
    "\n",
    "        return x,y.values,scaler\n",
    "\n",
    "    def loss_plot(self, history, plot_name = 'Loss'): # type(history) is dict\n",
    "        loss = np.asarray(history['loss'])\n",
    "        val_loss = np.asarray(history['val_loss'])\n",
    "\n",
    "        plt.style.use('seaborn')\n",
    "        plt.figure(figsize = (20,6), dpi=dpi)\n",
    "        plt.grid(True)\n",
    "        plt.plot(loss, color = 'darkgrey')\n",
    "        plt.plot(val_loss, color = 'tan')\n",
    "        plt.legend(['loss', 'val_loss'])\n",
    "        # plt.savefig('{}_{}_{}_{}_{}.png'.format(ticker, plot_name, str(n_epochs), str(time_steps), str(batch_size)))\n",
    "    \n",
    "    \n",
    "    def training_data_transform(self, ticker):\n",
    "        # Load train data\n",
    "        x, y, scaler = self.create_dataset(ticker)\n",
    "        self.x, self.y, self.scaler = x, y, scaler\n",
    "        print(\"Finished loading data.\")\n",
    "\n",
    "        with open(\"model/LSTMv2_scaler_{}_{}.p\".format(train_date_list[0],train_date_list[-1]),\"wb\") as f:\n",
    "            pickle.dump(scaler,f)\n",
    "\n",
    "    def model_training_testing(self, ticker, model, plot = False):\n",
    "        # Model Training pipeline\n",
    "        model_functionalities = Model_Functionalities(model)\n",
    "        \n",
    "        x, y, scaler = self.x, self.y, self.scaler\n",
    "        \n",
    "        if x is None:\n",
    "            print(\"None Training data processed\")\n",
    "            return\n",
    "        \n",
    "        # Build model, in-sample train test\n",
    "        train_history = model_functionalities.train_test(x, y, plot)  \n",
    "        if model_functionalities.model.model_name == \"LSTM\":\n",
    "            if plot == True:\n",
    "                self.loss_plot(train_history.history)\n",
    "\n",
    "        with open(\"model/LSTMv2_{}_{}.p\".format(train_date_list[0],train_date_list[-1]),\"wb\") as f:\n",
    "            pickle.dump(model,f)\n",
    "\n",
    "        # Out-of-sample test\n",
    "        for test_date in test_date_list:\n",
    "            # create test dateset\n",
    "            x_test, y_test, _ = self.create_dataset(ticker=ticker, dates=[test_date], time_steps = time_steps, input_scaler=scaler)\n",
    "            x_test, y_test = model_functionalities.model.reshape_dataset(x_test, y_test)\n",
    "\n",
    "            # use precious trained model to test\n",
    "            y_test_pred = model_functionalities.predict(x_test)\n",
    "            if model_functionalities.model.model_name == \"LSTM\":\n",
    "                if plot == True:\n",
    "                    model_functionalities.view_accuracy(y_test_pred.argmax(axis=1), y_test.argmax(axis=1))\n",
    "            if y_test.shape[1] != 1:\n",
    "                accuracy = np.mean(y_test_pred.argmax(axis=1)==y_test.argmax(axis=1))\n",
    "            else:\n",
    "                accuracy = np.mean(y_test_pred==y_test)\n",
    "            print(test_date+\" accuracy: \", accuracy)\n",
    "        return model_functionalities.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15154\n",
      "Finished loading data.\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline()\n",
    "pipeline.training_data_transform(ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LSTM to predict next up and down\n",
    "## v2: Predict up and downs of the average of $\\mathbf{nforward}=10$ following prices\n",
    "\n",
    "1. Too slow to predict (next trade may happen in millisecond)\n",
    "2. Only classification of up, down and same. No quantitative prediction (can be improved to predict quantity of price movement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_dim = 30\n",
    "n_epochs = 100\n",
    "batch_size = 128\n",
    "activation = \"tanh\"\n",
    "loss = 'categorical_crossentropy'\n",
    "stop_patience=20\n",
    "\n",
    "dpi=200\n",
    "\n",
    "\n",
    "class LSTM_Model():\n",
    "    def __init__(self):\n",
    "        self.model = Sequential()\n",
    "        self.model_name = \"LSTM\"\n",
    "        return \n",
    "    \n",
    "    def build(self,  time_steps = time_steps, data_dim = 1, output_dim = 3):\n",
    "        # expected input batch shape: (batch_size, timesteps, data_dim)\n",
    "        # the sample of index i in batch k is the follow-up for the sample i in batch k-1.\n",
    "        self.model.add(LSTM(hidden_dim, activation=activation, return_sequences=True, input_shape=(time_steps, data_dim)))\n",
    "        self.model.add(LSTM(hidden_dim, activation=activation, return_sequences=True))\n",
    "        self.model.add(LSTM(hidden_dim, activation=activation))\n",
    "        self.model.add(Dense(output_dim, activation = 'softmax'))\n",
    "        \n",
    "        opt=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "        self.model.compile(loss = loss, optimizer=opt, metrics=['accuracy']) \n",
    "        return self.model\n",
    "    \n",
    "    def fit(self, x_train, y_train, batch_size, epochs, validation_data, callbacks):\n",
    "        return self.model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, validation_data = validation_data, callbacks = callbacks)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)\n",
    "    \n",
    "    def reshape_dataset(self, x, y):\n",
    "        if x is not None:\n",
    "            if len(x.shape) == 2:\n",
    "                x = x.reshape(x.shape[0], x.shape[1], 1)\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(y.shape[0],1)\n",
    "        return x, y \n",
    "\n",
    "\n",
    "class Model_Functionalities():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    def train_test(self, x, y, plot = False):\n",
    "        \n",
    "        size = len(x)\n",
    "        if size!=len(y):\n",
    "            return None\n",
    "        x = x[: batch_size * (size // batch_size)]\n",
    "        y = y[: batch_size * (size // batch_size)]\n",
    "        \n",
    "        x, y = self.model.reshape_dataset(x, y)\n",
    "\n",
    "        x_train, x_validation, y_train, y_validation= train_test_split(x, y, test_size = 0.1, shuffle = False)\n",
    "        print('train', x_train.shape, y_train.shape)\n",
    "        print('validation', x_validation.shape, y_validation.shape)\n",
    "        \n",
    "        \n",
    "        if self.model.model_name == \"LSTM\":\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=stop_patience, mode=\"min\", verbose=2, restore_best_weights=True)\n",
    "            history = self.model.fit(x_train, y_train, batch_size = batch_size, epochs = n_epochs,\n",
    "                                     validation_data=(x_validation, y_validation),callbacks=[early_stopping])\n",
    "        else:\n",
    "            self.model.fit(x_train, y_train)\n",
    "        \n",
    "        self.y_pred = self.model.predict(x_validation)\n",
    "        self.y_validation_true = y_validation\n",
    "        \n",
    "        if plot == True:\n",
    "            if y.shape[1] != 1:\n",
    "                self.train_plot = self.view_accuracy(self.predict(x_train).argmax(axis=1), y_train.argmax(axis=1), 'Train')\n",
    "                self.validation_plot = self.view_accuracy(self.predict(x_validation).argmax(axis=1), y_validation.argmax(axis=1), 'Validation')\n",
    "            else:\n",
    "                self.train_plot = self.view_accuracy(self.predict(x_train), y_train, 'Train')\n",
    "                self.validation_plot = self.view_accuracy(self.predict(x_validation), y_validation, 'Validation')\n",
    "        if self.model.model_name == \"LSTM\":\n",
    "            return history\n",
    "\n",
    "    def predict(self, x_validation):\n",
    "        pred = self.model.predict(x_validation)\n",
    "        return pred\n",
    "    \n",
    "    def view_accuracy(self, y_pred = None, y_true = None, plot_name = 'Test', num=100):\n",
    "        if y_pred is None:\n",
    "            y_pred = self.y_pred.argmax(axis=1)\n",
    "            y_true = self.y_validation_true.argmax(axis=1)\n",
    "        \n",
    "        plt.style.use('seaborn')\n",
    "        plt.figure(figsize = (20,6), dpi=dpi)\n",
    "        plt.grid(True)\n",
    "        plt.plot(y_pred[:num], color = 'lightcoral')\n",
    "        plt.plot(y_true[:num], color = 'cornflowerblue', linewidth = 1)\n",
    "        plt.title('{}_{}'.format(ticker, plot_name))\n",
    "        plt.legend(['predict', 'true'])\n",
    "#         if plot_name == 'Test':\n",
    "#             plt.savefig('{}_{}_{}_{}.png'.format(ticker, plot_name, str(time_steps), str(batch_size)))\n",
    "#         else:\n",
    "#             plt.savefig('{}_{}_{}_{}.png'.format(ticker, plot_name, str(time_steps), str(batch_size)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (13593, 50, 4) (13593, 3)\n",
      "validation (1511, 50, 4) (1511, 3)\n",
      "Train on 13593 samples, validate on 1511 samples\n",
      "Epoch 1/100\n",
      "13593/13593 [==============================] - 12s 864us/step - loss: 0.8630 - acc: 0.4804 - val_loss: 0.8244 - val_acc: 0.4950\n",
      "Epoch 2/100\n",
      "13593/13593 [==============================] - 10s 736us/step - loss: 0.8142 - acc: 0.4987 - val_loss: 0.7968 - val_acc: 0.5202\n",
      "Epoch 3/100\n",
      "13593/13593 [==============================] - 10s 746us/step - loss: 0.7911 - acc: 0.5312 - val_loss: 0.7834 - val_acc: 0.5341\n",
      "Epoch 4/100\n",
      "13593/13593 [==============================] - 10s 751us/step - loss: 0.7678 - acc: 0.5713 - val_loss: 0.7626 - val_acc: 0.5625\n",
      "Epoch 5/100\n",
      "13593/13593 [==============================] - 11s 806us/step - loss: 0.7525 - acc: 0.5878 - val_loss: 0.7540 - val_acc: 0.5639\n",
      "Epoch 6/100\n",
      "13593/13593 [==============================] - 11s 806us/step - loss: 0.7477 - acc: 0.5866 - val_loss: 0.7500 - val_acc: 0.5685\n",
      "Epoch 7/100\n",
      "13593/13593 [==============================] - 12s 875us/step - loss: 0.7432 - acc: 0.5899 - val_loss: 0.7514 - val_acc: 0.5665\n",
      "Epoch 8/100\n",
      "13593/13593 [==============================] - 12s 861us/step - loss: 0.7402 - acc: 0.5894 - val_loss: 0.7447 - val_acc: 0.5784\n",
      "Epoch 9/100\n",
      "13593/13593 [==============================] - 11s 820us/step - loss: 0.7386 - acc: 0.5912 - val_loss: 0.7494 - val_acc: 0.5639\n",
      "Epoch 10/100\n",
      "13593/13593 [==============================] - 11s 833us/step - loss: 0.7352 - acc: 0.5932 - val_loss: 0.7399 - val_acc: 0.5711\n",
      "Epoch 11/100\n",
      "13593/13593 [==============================] - 14s 1ms/step - loss: 0.7343 - acc: 0.5971 - val_loss: 0.7445 - val_acc: 0.5619\n",
      "Epoch 12/100\n",
      "13593/13593 [==============================] - 13s 954us/step - loss: 0.7317 - acc: 0.5965 - val_loss: 0.7518 - val_acc: 0.5599\n",
      "Epoch 13/100\n",
      "13593/13593 [==============================] - 12s 855us/step - loss: 0.7308 - acc: 0.5966 - val_loss: 0.7414 - val_acc: 0.5751\n",
      "Epoch 14/100\n",
      "13593/13593 [==============================] - 11s 832us/step - loss: 0.7307 - acc: 0.5961 - val_loss: 0.7412 - val_acc: 0.5678\n",
      "Epoch 15/100\n",
      "13593/13593 [==============================] - 13s 936us/step - loss: 0.7294 - acc: 0.5966 - val_loss: 0.7405 - val_acc: 0.5619\n",
      "Epoch 16/100\n",
      "13593/13593 [==============================] - 13s 971us/step - loss: 0.7279 - acc: 0.5987 - val_loss: 0.7453 - val_acc: 0.5520\n",
      "Epoch 17/100\n",
      "13593/13593 [==============================] - 13s 948us/step - loss: 0.7274 - acc: 0.5990 - val_loss: 0.7391 - val_acc: 0.5678\n",
      "Epoch 18/100\n",
      "13593/13593 [==============================] - 13s 933us/step - loss: 0.7257 - acc: 0.6029 - val_loss: 0.7379 - val_acc: 0.5791\n",
      "Epoch 19/100\n",
      "13593/13593 [==============================] - 12s 869us/step - loss: 0.7256 - acc: 0.5974 - val_loss: 0.7388 - val_acc: 0.5606\n",
      "Epoch 20/100\n",
      "13593/13593 [==============================] - 14s 1ms/step - loss: 0.7271 - acc: 0.6005 - val_loss: 0.7421 - val_acc: 0.5725\n",
      "Epoch 21/100\n",
      "13593/13593 [==============================] - 14s 1ms/step - loss: 0.7237 - acc: 0.6031 - val_loss: 0.7385 - val_acc: 0.5639\n",
      "Epoch 22/100\n",
      "13593/13593 [==============================] - 13s 980us/step - loss: 0.7225 - acc: 0.6026 - val_loss: 0.7388 - val_acc: 0.5705\n",
      "Epoch 23/100\n",
      "13593/13593 [==============================] - 12s 876us/step - loss: 0.7223 - acc: 0.6033 - val_loss: 0.7370 - val_acc: 0.5738\n",
      "Epoch 24/100\n",
      "13593/13593 [==============================] - 12s 874us/step - loss: 0.7218 - acc: 0.6066 - val_loss: 0.7409 - val_acc: 0.5612\n",
      "Epoch 25/100\n",
      "13593/13593 [==============================] - 12s 866us/step - loss: 0.7207 - acc: 0.6066 - val_loss: 0.7360 - val_acc: 0.5711\n",
      "Epoch 26/100\n",
      "13593/13593 [==============================] - 14s 1ms/step - loss: 0.7193 - acc: 0.6081 - val_loss: 0.7491 - val_acc: 0.5579\n",
      "Epoch 27/100\n",
      "13593/13593 [==============================] - 13s 962us/step - loss: 0.7191 - acc: 0.6055 - val_loss: 0.7371 - val_acc: 0.5831\n",
      "Epoch 28/100\n",
      "13593/13593 [==============================] - 13s 976us/step - loss: 0.7170 - acc: 0.6110 - val_loss: 0.7517 - val_acc: 0.5559\n",
      "Epoch 29/100\n",
      "13593/13593 [==============================] - 15s 1ms/step - loss: 0.7160 - acc: 0.6108 - val_loss: 0.7413 - val_acc: 0.5625\n",
      "Epoch 30/100\n",
      "13593/13593 [==============================] - 14s 1ms/step - loss: 0.7150 - acc: 0.6141 - val_loss: 0.7396 - val_acc: 0.5751\n",
      "Epoch 31/100\n",
      "13593/13593 [==============================] - 14s 1ms/step - loss: 0.7146 - acc: 0.6120 - val_loss: 0.7447 - val_acc: 0.5526\n",
      "Epoch 32/100\n",
      "13593/13593 [==============================] - 13s 969us/step - loss: 0.7118 - acc: 0.6169 - val_loss: 0.7473 - val_acc: 0.5599\n",
      "Epoch 33/100\n",
      "13593/13593 [==============================] - 13s 988us/step - loss: 0.7111 - acc: 0.6176 - val_loss: 0.7455 - val_acc: 0.5645\n",
      "Epoch 34/100\n",
      "13593/13593 [==============================] - 12s 904us/step - loss: 0.7092 - acc: 0.6191 - val_loss: 0.7436 - val_acc: 0.5692\n",
      "Epoch 35/100\n",
      "13593/13593 [==============================] - 13s 920us/step - loss: 0.7080 - acc: 0.6168 - val_loss: 0.7435 - val_acc: 0.5659\n",
      "Epoch 36/100\n",
      "13593/13593 [==============================] - 12s 906us/step - loss: 0.7062 - acc: 0.6197 - val_loss: 0.7504 - val_acc: 0.5586\n",
      "Epoch 37/100\n",
      "13593/13593 [==============================] - 12s 920us/step - loss: 0.7065 - acc: 0.6151 - val_loss: 0.7437 - val_acc: 0.5612\n",
      "Epoch 38/100\n",
      "13593/13593 [==============================] - 15s 1ms/step - loss: 0.7036 - acc: 0.6227 - val_loss: 0.7557 - val_acc: 0.5506\n",
      "Epoch 39/100\n",
      "13593/13593 [==============================] - 15s 1ms/step - loss: 0.7002 - acc: 0.6236 - val_loss: 0.7521 - val_acc: 0.5559\n",
      "Epoch 40/100\n",
      "13593/13593 [==============================] - 14s 998us/step - loss: 0.6979 - acc: 0.6255 - val_loss: 0.7538 - val_acc: 0.5711\n",
      "Epoch 41/100\n",
      "13593/13593 [==============================] - 13s 932us/step - loss: 0.6972 - acc: 0.6295 - val_loss: 0.7494 - val_acc: 0.5645\n",
      "Epoch 42/100\n",
      "13593/13593 [==============================] - 14s 1ms/step - loss: 0.6944 - acc: 0.6277 - val_loss: 0.7650 - val_acc: 0.5592\n",
      "Epoch 43/100\n",
      "13593/13593 [==============================] - 15s 1ms/step - loss: 0.6926 - acc: 0.6304 - val_loss: 0.7571 - val_acc: 0.5645\n",
      "Epoch 44/100\n",
      "13593/13593 [==============================] - 14s 1ms/step - loss: 0.6906 - acc: 0.6355 - val_loss: 0.7563 - val_acc: 0.5579\n",
      "Epoch 45/100\n",
      "13593/13593 [==============================] - 13s 956us/step - loss: 0.6858 - acc: 0.6389 - val_loss: 0.7658 - val_acc: 0.5645\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00045: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190117 accuracy:  0.5866336633663366\n",
      "5342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190118 accuracy:  0.628790715087982\n",
      "2570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190122 accuracy:  0.6089494163424124\n",
      "2944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190123 accuracy:  0.6233016304347826\n",
      "1759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190124 accuracy:  0.5923820352472996\n",
      "1610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190125 accuracy:  0.5857142857142857\n",
      "1428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190128 accuracy:  0.5798319327731093\n",
      "1034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190129 accuracy:  0.5851063829787234\n",
      "2164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190130 accuracy:  0.583641404805915\n",
      "2679\n",
      "20190131 accuracy:  0.6129152668906308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.LSTM_Model at 0x18f308f60>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model = LSTM_Model()\n",
    "lstm_model.build(time_steps = time_steps, data_dim = pipeline.x.shape[-1], output_dim = pipeline.y.shape[-1])\n",
    "pipeline.model_training_testing(ticker, lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15154, 50, 4) (15154, 3)\n"
     ]
    }
   ],
   "source": [
    "print(pipeline.x.shape, pipeline.y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Conv2D, Flatten, Dropout\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 30\n",
    "n_epochs = 100\n",
    "batch_size = 128\n",
    "activation = \"tanh\"\n",
    "loss = 'categorical_crossentropy'\n",
    "stop_patience=20\n",
    "\n",
    "dpi=200\n",
    "\n",
    "\n",
    "class CNN_Model():\n",
    "    def __init__(self):\n",
    "        self.model = Sequential()\n",
    "        self.model_name = \"CNN\"\n",
    "        return \n",
    "    \n",
    "    def build(self, time_steps = time_steps, data_dim = 1, output_dim = 3):\n",
    "        #add model layers\n",
    "        self.model.add(Conv2D(28, kernel_size=2, activation='relu', input_shape=(time_steps, data_dim, 1)))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(Conv2D(28, kernel_size=2, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(output_dim, activation='softmax'))\n",
    "        \n",
    "        opt=keras.optimizers.Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "        self.model.compile(loss = loss, optimizer=opt, metrics=['accuracy']) \n",
    "        return self.model\n",
    "    \n",
    "    def fit(self, x_train, y_train, batch_size, epochs, validation_data, callbacks):\n",
    "        return self.model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, validation_data = validation_data, callbacks = callbacks)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)\n",
    "    \n",
    "    def reshape_dataset(self, x, y):\n",
    "        if x is not None:\n",
    "            if len(x.shape) == 3:\n",
    "                x = x.reshape(x.shape[0], x.shape[1], x.shape[2], 1)\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(y.shape[0],1)\n",
    "        return x, y \n",
    "\n",
    "\n",
    "class Model_Functionalities():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    def train_test(self, x, y, plot = False):\n",
    "        \n",
    "        size = len(x)\n",
    "        if size!=len(y):\n",
    "            return None\n",
    "        x = x[: batch_size * (size // batch_size)]\n",
    "        y = y[: batch_size * (size // batch_size)]\n",
    "        \n",
    "        x, y = self.model.reshape_dataset(x, y)\n",
    "\n",
    "        x_train, x_validation, y_train, y_validation= train_test_split(x, y, test_size = 0.1, shuffle = True)\n",
    "        print('train', x_train.shape, y_train.shape)\n",
    "        print('validation', x_validation.shape, y_validation.shape)\n",
    "        \n",
    "        \n",
    "        if self.model.model_name == \"LSTM\" or  self.model.model_name == \"CNN\":\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=stop_patience, mode=\"min\", verbose=2, restore_best_weights=True)\n",
    "            history = self.model.fit(x_train, y_train, batch_size = batch_size, epochs = n_epochs,\n",
    "                                     validation_data=(x_validation, y_validation),callbacks=[early_stopping])\n",
    "        else:\n",
    "            self.model.fit(x_train, y_train)\n",
    "        \n",
    "        self.y_pred = self.model.predict(x_validation)\n",
    "        self.y_validation_true = y_validation\n",
    "        \n",
    "        if plot == True:\n",
    "            if y.shape[1] != 1:\n",
    "                self.train_plot = self.view_accuracy(self.predict(x_train).argmax(axis=1), y_train.argmax(axis=1), 'Train')\n",
    "                self.validation_plot = self.view_accuracy(self.predict(x_validation).argmax(axis=1), y_validation.argmax(axis=1), 'Validation')\n",
    "            else:\n",
    "                self.train_plot = self.view_accuracy(self.predict(x_train), y_train, 'Train')\n",
    "                self.validation_plot = self.view_accuracy(self.predict(x_validation), y_validation, 'Validation')\n",
    "        if self.model.model_name == \"LSTM\":\n",
    "            return history\n",
    "\n",
    "    def predict(self, x_validation):\n",
    "        pred = self.model.predict(x_validation)\n",
    "        return pred\n",
    "    \n",
    "    def view_accuracy(self, y_pred = None, y_true = None, plot_name = 'Test', num=100):\n",
    "        if y_pred is None:\n",
    "            y_pred = self.y_pred.argmax(axis=1)\n",
    "            y_true = self.y_validation_true.argmax(axis=1)\n",
    "        \n",
    "        plt.style.use('seaborn')\n",
    "        plt.figure(figsize = (20,6), dpi=dpi)\n",
    "        plt.grid(True)\n",
    "        plt.plot(y_pred[:num], color = 'lightcoral')\n",
    "        plt.plot(y_true[:num], color = 'cornflowerblue', linewidth = 1)\n",
    "        plt.title('{}_{}'.format(ticker, plot_name))\n",
    "        plt.legend(['predict', 'true'])\n",
    "#         if plot_name == 'Test':\n",
    "#             plt.savefig('{}_{}_{}_{}.png'.format(ticker, plot_name, str(time_steps), str(batch_size)))\n",
    "#         else:\n",
    "#             plt.savefig('{}_{}_{}_{}.png'.format(ticker, plot_name, str(time_steps), str(batch_size)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (13593, 50, 4, 1) (13593, 3)\n",
      "validation (1511, 50, 4, 1) (1511, 3)\n",
      "Train on 13593 samples, validate on 1511 samples\n",
      "Epoch 1/100\n",
      "13593/13593 [==============================] - 4s 274us/step - loss: 0.8441 - acc: 0.5083 - val_loss: 0.8301 - val_acc: 0.5619\n",
      "Epoch 2/100\n",
      "13593/13593 [==============================] - 2s 119us/step - loss: 0.7914 - acc: 0.5605 - val_loss: 0.8095 - val_acc: 0.5659\n",
      "Epoch 3/100\n",
      "13593/13593 [==============================] - 2s 120us/step - loss: 0.7754 - acc: 0.5719 - val_loss: 0.7990 - val_acc: 0.5804\n",
      "Epoch 4/100\n",
      "13593/13593 [==============================] - 2s 120us/step - loss: 0.7651 - acc: 0.5779 - val_loss: 0.7846 - val_acc: 0.5890\n",
      "Epoch 5/100\n",
      "13593/13593 [==============================] - 2s 119us/step - loss: 0.7597 - acc: 0.5808 - val_loss: 0.7896 - val_acc: 0.5817\n",
      "Epoch 6/100\n",
      "13593/13593 [==============================] - 2s 129us/step - loss: 0.7580 - acc: 0.5767 - val_loss: 0.7849 - val_acc: 0.5745\n",
      "Epoch 7/100\n",
      "13593/13593 [==============================] - 2s 132us/step - loss: 0.7521 - acc: 0.5885 - val_loss: 0.7871 - val_acc: 0.5804\n",
      "Epoch 8/100\n",
      "13593/13593 [==============================] - 2s 131us/step - loss: 0.7544 - acc: 0.5824 - val_loss: 0.7862 - val_acc: 0.5837\n",
      "Epoch 9/100\n",
      "13593/13593 [==============================] - 2s 123us/step - loss: 0.7472 - acc: 0.5924 - val_loss: 0.7781 - val_acc: 0.5791\n",
      "Epoch 10/100\n",
      "13593/13593 [==============================] - 2s 125us/step - loss: 0.7451 - acc: 0.5916 - val_loss: 0.7804 - val_acc: 0.5877\n",
      "Epoch 11/100\n",
      "13593/13593 [==============================] - 2s 126us/step - loss: 0.7433 - acc: 0.5929 - val_loss: 0.7781 - val_acc: 0.5884\n",
      "Epoch 12/100\n",
      "13593/13593 [==============================] - 2s 125us/step - loss: 0.7438 - acc: 0.5985 - val_loss: 0.7773 - val_acc: 0.5758\n",
      "Epoch 13/100\n",
      "13593/13593 [==============================] - 2s 125us/step - loss: 0.7405 - acc: 0.5959 - val_loss: 0.7886 - val_acc: 0.5778\n",
      "Epoch 14/100\n",
      "13593/13593 [==============================] - 2s 125us/step - loss: 0.7386 - acc: 0.5992 - val_loss: 0.7834 - val_acc: 0.5870\n",
      "Epoch 15/100\n",
      "13593/13593 [==============================] - 2s 128us/step - loss: 0.7440 - acc: 0.5950 - val_loss: 0.7919 - val_acc: 0.5811\n",
      "Epoch 16/100\n",
      "13593/13593 [==============================] - 2s 138us/step - loss: 0.7405 - acc: 0.5969 - val_loss: 0.7869 - val_acc: 0.5771\n",
      "Epoch 17/100\n",
      "13593/13593 [==============================] - 2s 129us/step - loss: 0.7368 - acc: 0.5988 - val_loss: 0.7847 - val_acc: 0.5943\n",
      "Epoch 18/100\n",
      "13593/13593 [==============================] - 2s 135us/step - loss: 0.7365 - acc: 0.6023 - val_loss: 0.7862 - val_acc: 0.5745\n",
      "Epoch 19/100\n",
      "13593/13593 [==============================] - 2s 130us/step - loss: 0.7359 - acc: 0.6002 - val_loss: 0.7883 - val_acc: 0.5718\n",
      "Epoch 20/100\n",
      "13593/13593 [==============================] - 2s 128us/step - loss: 0.7307 - acc: 0.6046 - val_loss: 0.7848 - val_acc: 0.5870\n",
      "Epoch 21/100\n",
      "13593/13593 [==============================] - 2s 134us/step - loss: 0.7311 - acc: 0.6079 - val_loss: 0.7857 - val_acc: 0.5678\n",
      "Epoch 22/100\n",
      "13593/13593 [==============================] - 2s 126us/step - loss: 0.7334 - acc: 0.6033 - val_loss: 0.7758 - val_acc: 0.5804\n",
      "Epoch 23/100\n",
      "13593/13593 [==============================] - 2s 126us/step - loss: 0.7311 - acc: 0.6038 - val_loss: 0.7819 - val_acc: 0.5831\n",
      "Epoch 24/100\n",
      "13593/13593 [==============================] - 2s 129us/step - loss: 0.7301 - acc: 0.6074 - val_loss: 0.7817 - val_acc: 0.5711\n",
      "Epoch 25/100\n",
      "13593/13593 [==============================] - 2s 134us/step - loss: 0.7307 - acc: 0.6091 - val_loss: 0.7858 - val_acc: 0.5764\n",
      "Epoch 26/100\n",
      "13593/13593 [==============================] - 2s 129us/step - loss: 0.7269 - acc: 0.6071 - val_loss: 0.7868 - val_acc: 0.5764\n",
      "Epoch 27/100\n",
      "13593/13593 [==============================] - 2s 130us/step - loss: 0.7301 - acc: 0.6053 - val_loss: 0.7883 - val_acc: 0.5745\n",
      "Epoch 28/100\n",
      "13593/13593 [==============================] - 2s 124us/step - loss: 0.7312 - acc: 0.6060 - val_loss: 0.7853 - val_acc: 0.5817\n",
      "Epoch 29/100\n",
      "13593/13593 [==============================] - 2s 130us/step - loss: 0.7307 - acc: 0.6069 - val_loss: 0.7874 - val_acc: 0.5685\n",
      "Epoch 30/100\n",
      "13593/13593 [==============================] - 2s 130us/step - loss: 0.7265 - acc: 0.6085 - val_loss: 0.7929 - val_acc: 0.5817\n",
      "Epoch 31/100\n",
      "13593/13593 [==============================] - 2s 132us/step - loss: 0.7258 - acc: 0.6097 - val_loss: 0.7853 - val_acc: 0.5917\n",
      "Epoch 32/100\n",
      "13593/13593 [==============================] - 2s 127us/step - loss: 0.7264 - acc: 0.6066 - val_loss: 0.7832 - val_acc: 0.5745\n",
      "Epoch 33/100\n",
      "13593/13593 [==============================] - 2s 127us/step - loss: 0.7225 - acc: 0.6119 - val_loss: 0.7786 - val_acc: 0.5837\n",
      "Epoch 34/100\n",
      "13593/13593 [==============================] - 2s 131us/step - loss: 0.7262 - acc: 0.6119 - val_loss: 0.7949 - val_acc: 0.5811\n",
      "Epoch 35/100\n",
      "13593/13593 [==============================] - 2s 129us/step - loss: 0.7261 - acc: 0.6080 - val_loss: 0.7853 - val_acc: 0.5923\n",
      "Epoch 36/100\n",
      "13593/13593 [==============================] - 2s 129us/step - loss: 0.7263 - acc: 0.6074 - val_loss: 0.7823 - val_acc: 0.5850\n",
      "Epoch 37/100\n",
      "13593/13593 [==============================] - 2s 136us/step - loss: 0.7271 - acc: 0.6034 - val_loss: 0.7881 - val_acc: 0.5745\n",
      "Epoch 38/100\n",
      "13593/13593 [==============================] - 2s 142us/step - loss: 0.7272 - acc: 0.6082 - val_loss: 0.7922 - val_acc: 0.5844\n",
      "Epoch 39/100\n",
      "13593/13593 [==============================] - 2s 141us/step - loss: 0.7200 - acc: 0.6143 - val_loss: 0.7833 - val_acc: 0.5791\n",
      "Epoch 40/100\n",
      "13593/13593 [==============================] - 2s 134us/step - loss: 0.7242 - acc: 0.6188 - val_loss: 0.7875 - val_acc: 0.5751\n",
      "Epoch 41/100\n",
      "13593/13593 [==============================] - 2s 131us/step - loss: 0.7248 - acc: 0.6090 - val_loss: 0.7956 - val_acc: 0.5751\n",
      "Epoch 42/100\n",
      "13593/13593 [==============================] - 2s 129us/step - loss: 0.7199 - acc: 0.6171 - val_loss: 0.7876 - val_acc: 0.5811\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00042: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808\n",
      "20190117 accuracy:  0.5742574257425742\n",
      "5342\n",
      "20190118 accuracy:  0.6149382253837514\n",
      "2570\n",
      "20190122 accuracy:  0.5988326848249027\n",
      "2944\n",
      "20190123 accuracy:  0.6019021739130435\n",
      "1759\n",
      "20190124 accuracy:  0.572484366117112\n",
      "1610\n",
      "20190125 accuracy:  0.5832298136645963\n",
      "1428\n",
      "20190128 accuracy:  0.5777310924369747\n",
      "1034\n",
      "20190129 accuracy:  0.5831721470019342\n",
      "2164\n",
      "20190130 accuracy:  0.5637707948243993\n",
      "2679\n",
      "20190131 accuracy:  0.5834266517357223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.CNN_Model at 0x19746e5f8>"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model = CNN_Model()\n",
    "cnn_model.build(time_steps = time_steps, data_dim = pipeline.x.shape[-1], output_dim = pipeline.y.shape[-1])\n",
    "pipeline.model_training_testing(ticker, cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self,max_depth=50, n_estimators=100, max_features=0.36, criterion = 'gini'):\n",
    "        self.model = RandomForestClassifier(n_estimators=n_estimators, criterion = criterion, max_depth=max_depth,max_features=max_features)\n",
    "        self.model_name = \"Random_Forest\"\n",
    "    def fit(self,X,Y):\n",
    "        return self.model.fit(X,Y)\n",
    "    def predict(self,X):\n",
    "        return self.model.predict(X)\n",
    "    def reshape_dataset(self, x, y):\n",
    "        if x is not None:\n",
    "            if len(x.shape) == 3:\n",
    "                x = x.reshape(x.shape[0], x.shape[1]*x.shape[2])\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(y.shape[0],1)\n",
    "        return x, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (13593, 200) (13593, 3)\n",
      "validation (1511, 200) (1511, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808\n",
      "20190117 accuracy:  0.5618811881188119\n",
      "5342\n",
      "20190118 accuracy:  0.6042680643953575\n",
      "2570\n",
      "20190122 accuracy:  0.5622568093385214\n",
      "2944\n",
      "20190123 accuracy:  0.553328804347826\n",
      "1759\n",
      "20190124 accuracy:  0.533826037521319\n",
      "1610\n",
      "20190125 accuracy:  0.5391304347826087\n",
      "1428\n",
      "20190128 accuracy:  0.5301120448179272\n",
      "1034\n",
      "20190129 accuracy:  0.5435203094777563\n",
      "2164\n",
      "20190130 accuracy:  0.5619223659889094\n",
      "2679\n",
      "20190131 accuracy:  0.5718551698394924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RandomForest at 0x184ba23c8>"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_model = RandomForest()\n",
    "pipeline.model_training_testing(ticker, random_forest_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoost():\n",
    "    def __init__(self):\n",
    "        self.model = GradientBoostingClassifier(n_estimators=100,max_depth=50,learning_rate=0.05)\n",
    "        self.model_name = \"GradientBoost\"\n",
    "    def fit(self,X,Y):\n",
    "        return self.model.fit(X,Y)\n",
    "    def predict(self,X):\n",
    "        return self.model.predict(X)\n",
    "    def reshape_dataset(self, x, y):\n",
    "        if x is not None:\n",
    "            if len(x.shape) == 3:\n",
    "                x = x.reshape(x.shape[0], x.shape[1]*x.shape[2])\n",
    "        if len(y.shape) != 1:\n",
    "            y = np.where(y==1)[1]\n",
    "            y = y.reshape(y.shape[0],1)\n",
    "        return x, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (13593, 200) (13593, 1)\n",
      "validation (1511, 200) (1511, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/_gb.py:1454: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808\n",
      "20190117 accuracy:  0.465444564258406\n",
      "5342\n",
      "20190118 accuracy:  0.43963022835926063\n",
      "2570\n",
      "20190122 accuracy:  0.4527048100652546\n",
      "2944\n",
      "20190123 accuracy:  0.45738938445179583\n",
      "1759\n",
      "20190124 accuracy:  0.4616870081940324\n",
      "1610\n",
      "20190125 accuracy:  0.46697966899425175\n",
      "1428\n",
      "20190128 accuracy:  0.4662188404773674\n",
      "1034\n",
      "20190129 accuracy:  0.4693851972958109\n",
      "2164\n",
      "20190130 accuracy:  0.46788354898336415\n",
      "2679\n",
      "20190131 accuracy:  0.46980378125191147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.GradientBoost at 0x137823dd8>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_boost_model=GradientBoost()\n",
    "pipeline.model_training_testing(ticker, gradient_boost_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class XGBoost():\n",
    "    def __init__(self):\n",
    "        self.model =XGBClassifier(n_estimators=100, max_depth=50,learning_rate=0.1,reg_lambda=0.1, verbose=True)\n",
    "        self.model_name = \"XGBoost\"\n",
    "    def fit(self,X,Y):\n",
    "        return self.model.fit(X,Y)\n",
    "    def predict(self,X):\n",
    "        return self.model.predict(X)\n",
    "    def reshape_dataset(self, x, y):\n",
    "        if x is not None:\n",
    "            if len(x.shape) == 3:\n",
    "                x = x.reshape(x.shape[0], x.shape[1]*x.shape[2])\n",
    "        if len(y.shape) != 1:\n",
    "            y = np.where(y==1)[1]\n",
    "            y = y.reshape(y.shape[0],1)\n",
    "        return x, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (13593, 200) (13593, 1)\n",
      "validation (1511, 200) (1511, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808\n",
      "20190117 accuracy:  0.47681140329379473\n",
      "5342\n",
      "20190118 accuracy:  0.4400565175748899\n",
      "2570\n",
      "20190122 accuracy:  0.45910309013005496\n",
      "2944\n",
      "20190123 accuracy:  0.45345763583264415\n",
      "1759\n",
      "20190124 accuracy:  0.46595677359448573\n",
      "1610\n",
      "20190125 accuracy:  0.48326029088383937\n",
      "1428\n",
      "20190128 accuracy:  0.4760521855801144\n",
      "1034\n",
      "20190129 accuracy:  0.48628450852822225\n",
      "2164\n",
      "20190130 accuracy:  0.47385293203180257\n",
      "2679\n",
      "20190131 accuracy:  0.473853361016051\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.XGBoost at 0x14d288710>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost_model=XGBoost()\n",
    "pipeline.model_training_testing(ticker, xgboost_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Tick Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TICK FACTOR\n",
    "# only update if it's a trade\n",
    "# if message_type == 't':\n",
    "#     # calc the tick\n",
    "#     this_tick = np.sign(last_price - prev_price)\n",
    "#     if this_tick == 0:\n",
    "#         this_tick = prev_tick\n",
    "\n",
    "#     # now calc the tick\n",
    "#     if tick_factor == 0:\n",
    "#         tick_factor = this_tick\n",
    "#     else:\n",
    "#         tick_factor = (tick_ema_alpha * this_tick) + (1 - tick_ema_alpha) * tick_factor\n",
    "\n",
    "#         # store the last tick\n",
    "#     prev_tick = this_tick\n",
    "\n",
    "for test_date in test_date_list:\n",
    "    df = pd.read_csv(data_dir+'trades_{}_{}.csv'.format(test_date, ticker),index_col=[0],parse_dates=[0])\n",
    "    \n",
    "    df[\"tick_test\"]=(df[\"trade_px\"]-df[\"trade_px\"].shift(1)).apply(lambda x: 1 if x>0. else (-1. if x<0 else np.nan))\n",
    "    df.fillna(method=\"ffill\",inplace=True)\n",
    "    \n",
    "    df[\"tick_factor\"]=df[\"tick_test\"].ewm(span=20).mean()\n",
    "    df.dropna(axis=0,inplace=True)\n",
    "    \n",
    "    mysign = lambda x: 0 if abs(x)<1e-5 else (1 if x>0 else -1)\n",
    "    df[\"predict\"]=df[\"tick_factor\"].apply(mysign)\n",
    "    df[\"real_movement\"]=(df[\"trade_px\"].rolling(nforward).mean().shift(-nforward)-df[\"trade_px\"]).apply(mysign)\n",
    "    \n",
    "    df.dropna(axis=0,inplace=True)\n",
    "    acc=np.mean(df[\"predict\"]==df[\"real_movement\"])\n",
    "    print(\"Accuracy of {}\".format(test_date),acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum=pd.Timedelta(0)\n",
    "count=0\n",
    "for test_date in test_date_list:\n",
    "    df = pd.read_csv(data_dir+'trades_{}_{}.csv'.format(test_date, ticker),index_col=[0],parse_dates=[0])\n",
    "    timestamp=pd.DataFrame({\"trade_time\":df.index})\n",
    "    dt=timestamp[\"trade_time\"].shift(-nforward)-timestamp[\"trade_time\"]\n",
    "    dt.dropna(axis=0,inplace=True)\n",
    "    dt.apply(lambda x:x.seconds).hist()\n",
    "    sum+=dt.sum()\n",
    "    count+=dt.shape[0]\n",
    "print(\"Average time interval between {} trades is\".format(nforward),sum/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
