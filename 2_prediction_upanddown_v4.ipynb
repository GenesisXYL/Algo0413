{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifications:\n",
    "\n",
    "1. Turn different models into another file, and VWAP should import the models to read models.\n",
    "2. Rewrite VWAP LSTM factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir=\"./data/\"\n",
    "ticker=\"TSLA\"\n",
    "\n",
    "date_pool=pd.date_range(\"1/1/2019\",\"1/31/2019\",freq=\"B\").strftime(\"%Y%m%d\")\n",
    "date_pool=[d for d in date_pool if os.path.exists(data_dir+\"trades_{}_{}.csv\".format(d,ticker))]\n",
    "\n",
    "train_days=10\n",
    "train_date_list=date_pool[:train_days]\n",
    "test_date_list=date_pool[train_days+1:]\n",
    "time_steps = 50\n",
    "\n",
    "nforward=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline():\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.scaler = None\n",
    "    def load_data(self, ticker, date):\n",
    "        df = pd.read_csv(data_dir+'trades_{}_{}.csv'.format(date, ticker),index_col=[0],parse_dates=[0])\n",
    "\n",
    "        # Feature Engineering\n",
    "        df[\"direction\"]=(df[\"trade_px\"]-df[\"trade_px\"].shift(1)).apply(np.sign)\n",
    "        df[\"pct_change\"]=df[\"trade_px\"].pct_change()\n",
    "\n",
    "        mysign=lambda x: 0 if abs(x)<1e-5 else (1 if x>0 else -1)\n",
    "        df[\"label\"]=(df[\"trade_px\"].rolling(nforward).mean().shift(-nforward)-df[\"trade_px\"]).apply(mysign)\n",
    "        # df[\"label\"]=(df[\"trade_px\"].shift(-1)-df[\"trade_px\"]).apply(np.sign) # last version\n",
    "\n",
    "        df.fillna(method=\"ffill\",inplace=True)\n",
    "        df.dropna(axis=0,inplace=True)\n",
    "        # print(df.head(10),df.shape)\n",
    "        # print(\"NaN number: \",df.isna().sum().sum())\n",
    "\n",
    "        return df[[\"trade_px\",\"trade_size\",\"pct_change\",\"direction\",\"label\"]].values\n",
    "\n",
    "    def create_dataset(self, ticker=ticker, dates=train_date_list, time_steps = time_steps, input_scaler=None):  \n",
    "        for i,d in enumerate(dates):\n",
    "            datanew = self.load_data(ticker,d)\n",
    "            if i==0:\n",
    "                data=datanew\n",
    "            else:\n",
    "                data=np.vstack((data, datanew))\n",
    "\n",
    "        label=data[:,-1]\n",
    "        data=data[:,:-1]\n",
    "\n",
    "        if input_scaler is None:\n",
    "            scaler=StandardScaler()\n",
    "            data=scaler.fit_transform(data)\n",
    "        else:\n",
    "            data=input_scaler.transform(data)\n",
    "            scaler=input_scaler\n",
    "\n",
    "        x = [data[0 : time_steps]]\n",
    "        y = [label[time_steps-1]]\n",
    "        N=len(data)//time_steps\n",
    "\n",
    "        print(N)\n",
    "        for i in range(1, N):\n",
    "            t = data[i*time_steps: (i + 1)*time_steps]\n",
    "            x = np.vstack((x, [t]))\n",
    "            y.append(label[(i + 1)*time_steps-1])\n",
    "\n",
    "        y=pd.get_dummies(y)\n",
    "        #print(y)\n",
    "\n",
    "        return x,y.values,scaler\n",
    "\n",
    "    def loss_plot(self, history, plot_name = 'Loss'): # type(history) is dict\n",
    "        loss = np.asarray(history['loss'])\n",
    "        val_loss = np.asarray(history['val_loss'])\n",
    "\n",
    "        plt.style.use('seaborn')\n",
    "        plt.figure(figsize = (20,6), dpi=dpi)\n",
    "        plt.grid(True)\n",
    "        plt.plot(loss, color = 'darkgrey')\n",
    "        plt.plot(val_loss, color = 'tan')\n",
    "        plt.legend(['loss', 'val_loss'])\n",
    "        # plt.savefig('{}_{}_{}_{}_{}.png'.format(ticker, plot_name, str(n_epochs), str(time_steps), str(batch_size)))\n",
    "    \n",
    "    \n",
    "    def training_data_transform(self, ticker):\n",
    "        # Load train data\n",
    "        x, y, scaler = self.create_dataset(ticker)\n",
    "        self.x, self.y, self.scaler = x, y, scaler\n",
    "        print(\"Finished loading data.\")\n",
    "\n",
    "        with open(\"model/scaler_{}_{}.p\".format(train_date_list[0],train_date_list[-1]),\"wb\") as f:\n",
    "            pickle.dump(scaler,f)\n",
    "\n",
    "    def model_training_testing(self, ticker, model, plot = False):\n",
    "        # Model Training pipeline\n",
    "        model_functionalities = Model_Functionalities(model)\n",
    "        \n",
    "        x, y, scaler = self.x, self.y, self.scaler\n",
    "        print(y[:10])\n",
    "        \n",
    "        if x is None:\n",
    "            print(\"None Training data processed\")\n",
    "            return\n",
    "        \n",
    "        # Build model, in-sample train test\n",
    "        train_history = model_functionalities.train_test(x, y, plot)  \n",
    "        if model_functionalities.model.model_name == \"LSTM\":\n",
    "            if plot == True:\n",
    "                self.loss_plot(train_history.history)\n",
    "\n",
    "        with open(\"model/\" + model.model_name + \"_{}_{}.p\".format(train_date_list[0],train_date_list[-1]),\"wb\") as f:\n",
    "            pickle.dump(model_functionalities.model,f)\n",
    "\n",
    "        # Out-of-sample test\n",
    "        for test_date in test_date_list:\n",
    "            # create test dateset\n",
    "            x_test, y_test, _ = self.create_dataset(ticker=ticker, dates=[test_date], time_steps = time_steps, input_scaler=scaler)\n",
    "            x_test, y_test = model_functionalities.model.reshape_dataset(x_test, y_test)\n",
    "\n",
    "            # use precious trained model to test\n",
    "            y_test_pred = model_functionalities.predict(x_test)\n",
    "            if model_functionalities.model.model_name == \"LSTM\":\n",
    "                if plot == True:\n",
    "                    model_functionalities.view_accuracy(y_test_pred.argmax(axis=1), y_test.argmax(axis=1))\n",
    "            if y_test.shape[1] != 1:\n",
    "                accuracy = np.mean(y_test_pred.argmax(axis=1)==y_test.argmax(axis=1))\n",
    "            else:\n",
    "                accuracy = np.mean(y_test_pred==y_test)\n",
    "            print(test_date+\" accuracy: \", accuracy)\n",
    "        return model_functionalities.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15154\n",
      "Finished loading data.\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline()\n",
    "pipeline.training_data_transform(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Functionalities():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    def train_test(self, x, y, plot = False):\n",
    "        \n",
    "        size = len(x)\n",
    "        if size!=len(y):\n",
    "            return None\n",
    "        x = x[: batch_size * (size // batch_size)]\n",
    "        y = y[: batch_size * (size // batch_size)]\n",
    "        \n",
    "        x, y = self.model.reshape_dataset(x, y)\n",
    "\n",
    "        x_train, x_validation, y_train, y_validation= train_test_split(x, y, test_size = 0.1, shuffle = True)\n",
    "        print('train', x_train.shape, y_train.shape)\n",
    "        print('validation', x_validation.shape, y_validation.shape)\n",
    "        \n",
    "        \n",
    "        if self.model.model_name == \"LSTM\" or  self.model.model_name == \"CNN\":\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=stop_patience, mode=\"min\", verbose=2, restore_best_weights=True)\n",
    "            history = self.model.fit(x_train, y_train, batch_size = batch_size, epochs = n_epochs,\n",
    "                                     validation_data=(x_validation, y_validation),callbacks=[early_stopping])\n",
    "        else:\n",
    "            self.model.fit(x_train, y_train)\n",
    "        \n",
    "        self.y_pred = self.model.predict(x_validation)\n",
    "        self.y_validation_true = y_validation\n",
    "        \n",
    "        if plot == True:\n",
    "            if y.shape[1] != 1:\n",
    "                self.train_plot = self.view_accuracy(self.predict(x_train).argmax(axis=1), y_train.argmax(axis=1), 'Train')\n",
    "                self.validation_plot = self.view_accuracy(self.predict(x_validation).argmax(axis=1), y_validation.argmax(axis=1), 'Validation')\n",
    "            else:\n",
    "                self.train_plot = self.view_accuracy(self.predict(x_train), y_train, 'Train')\n",
    "                self.validation_plot = self.view_accuracy(self.predict(x_validation), y_validation, 'Validation')\n",
    "        if self.model.model_name == \"LSTM\":\n",
    "            return history\n",
    "\n",
    "    def predict(self, x_validation):\n",
    "        pred = self.model.predict(x_validation)\n",
    "        return pred\n",
    "    \n",
    "    def view_accuracy(self, y_pred = None, y_true = None, plot_name = 'Test', num=100):\n",
    "        if y_pred is None:\n",
    "            y_pred = self.y_pred.argmax(axis=1)\n",
    "            y_true = self.y_validation_true.argmax(axis=1)\n",
    "        \n",
    "        plt.style.use('seaborn')\n",
    "        plt.figure(figsize = (20,6), dpi=dpi)\n",
    "        plt.grid(True)\n",
    "        plt.plot(y_pred[:num], color = 'lightcoral')\n",
    "        plt.plot(y_true[:num], color = 'cornflowerblue', linewidth = 1)\n",
    "        plt.title('{}_{}'.format(ticker, plot_name))\n",
    "        plt.legend(['predict', 'true'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LSTM to predict next up and down\n",
    "## v2: Predict up and downs of the average of $\\mathbf{nforward}=10$ following prices\n",
    "\n",
    "1. Too slow to predict (next trade may happen in millisecond)\n",
    "2. Only classification of up, down and same. No quantitative prediction (can be improved to predict quantity of price movement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 30\n",
    "n_epochs = 100\n",
    "batch_size = 128\n",
    "activation = \"tanh\"\n",
    "loss = 'categorical_crossentropy'\n",
    "stop_patience=20\n",
    "\n",
    "dpi=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n",
      "train (13593, 50, 4) (13593, 3)\n",
      "validation (1511, 50, 4) (1511, 3)\n",
      "Train on 13593 samples, validate on 1511 samples\n",
      "Epoch 1/100\n",
      "13593/13593 [==============================] - 30s 2ms/step - loss: 0.8598 - accuracy: 0.5047 - val_loss: 0.8422 - val_accuracy: 0.5122\n",
      "Epoch 2/100\n",
      "13593/13593 [==============================] - 30s 2ms/step - loss: 0.8131 - accuracy: 0.5559 - val_loss: 0.8138 - val_accuracy: 0.5625\n",
      "Epoch 3/100\n",
      "13593/13593 [==============================] - 29s 2ms/step - loss: 0.7739 - accuracy: 0.5731 - val_loss: 0.7625 - val_accuracy: 0.5797\n",
      "Epoch 4/100\n",
      "13593/13593 [==============================] - 35s 3ms/step - loss: 0.7564 - accuracy: 0.5821 - val_loss: 0.7578 - val_accuracy: 0.5771\n",
      "Epoch 5/100\n",
      "13593/13593 [==============================] - 31s 2ms/step - loss: 0.7513 - accuracy: 0.5814 - val_loss: 0.7540 - val_accuracy: 0.5738\n",
      "Epoch 6/100\n",
      "13593/13593 [==============================] - 31s 2ms/step - loss: 0.7457 - accuracy: 0.5871 - val_loss: 0.7520 - val_accuracy: 0.5817\n",
      "Epoch 7/100\n",
      "13593/13593 [==============================] - 32s 2ms/step - loss: 0.7425 - accuracy: 0.5898 - val_loss: 0.7416 - val_accuracy: 0.5864\n",
      "Epoch 8/100\n",
      "13593/13593 [==============================] - 31s 2ms/step - loss: 0.7398 - accuracy: 0.5884 - val_loss: 0.7406 - val_accuracy: 0.5890\n",
      "Epoch 9/100\n",
      "13593/13593 [==============================] - 33s 2ms/step - loss: 0.7375 - accuracy: 0.5921 - val_loss: 0.7396 - val_accuracy: 0.5877\n",
      "Epoch 10/100\n",
      "13593/13593 [==============================] - 34s 2ms/step - loss: 0.7366 - accuracy: 0.5923 - val_loss: 0.7396 - val_accuracy: 0.5884\n",
      "Epoch 11/100\n",
      "13593/13593 [==============================] - 37s 3ms/step - loss: 0.7349 - accuracy: 0.5953 - val_loss: 0.7383 - val_accuracy: 0.5930\n",
      "Epoch 12/100\n",
      "13593/13593 [==============================] - 38s 3ms/step - loss: 0.7346 - accuracy: 0.5917 - val_loss: 0.7362 - val_accuracy: 0.5943\n",
      "Epoch 13/100\n",
      "13593/13593 [==============================] - 38s 3ms/step - loss: 0.7328 - accuracy: 0.5954 - val_loss: 0.7385 - val_accuracy: 0.5831\n",
      "Epoch 14/100\n",
      "13593/13593 [==============================] - 35s 3ms/step - loss: 0.7310 - accuracy: 0.5955 - val_loss: 0.7377 - val_accuracy: 0.5884\n",
      "Epoch 15/100\n",
      "13593/13593 [==============================] - 35s 3ms/step - loss: 0.7305 - accuracy: 0.5968 - val_loss: 0.7360 - val_accuracy: 0.5804\n",
      "Epoch 16/100\n",
      "13593/13593 [==============================] - 35s 3ms/step - loss: 0.7297 - accuracy: 0.5963 - val_loss: 0.7333 - val_accuracy: 0.6009\n",
      "Epoch 17/100\n",
      "13593/13593 [==============================] - 36s 3ms/step - loss: 0.7293 - accuracy: 0.5981 - val_loss: 0.7334 - val_accuracy: 0.5943\n",
      "Epoch 18/100\n",
      "13593/13593 [==============================] - 38s 3ms/step - loss: 0.7283 - accuracy: 0.5986 - val_loss: 0.7427 - val_accuracy: 0.5771\n",
      "Epoch 19/100\n",
      "13593/13593 [==============================] - 34s 3ms/step - loss: 0.7283 - accuracy: 0.5991 - val_loss: 0.7361 - val_accuracy: 0.5930\n",
      "Epoch 20/100\n",
      "13593/13593 [==============================] - 34s 3ms/step - loss: 0.7262 - accuracy: 0.5990 - val_loss: 0.7343 - val_accuracy: 0.5930\n",
      "Epoch 21/100\n",
      "13593/13593 [==============================] - 35s 3ms/step - loss: 0.7264 - accuracy: 0.6015 - val_loss: 0.7346 - val_accuracy: 0.5804\n",
      "Epoch 22/100\n",
      "13593/13593 [==============================] - 34s 2ms/step - loss: 0.7254 - accuracy: 0.6035 - val_loss: 0.7415 - val_accuracy: 0.5917\n",
      "Epoch 23/100\n",
      "13593/13593 [==============================] - 38s 3ms/step - loss: 0.7244 - accuracy: 0.6038 - val_loss: 0.7363 - val_accuracy: 0.5923\n",
      "Epoch 24/100\n",
      "13593/13593 [==============================] - 37s 3ms/step - loss: 0.7247 - accuracy: 0.6006 - val_loss: 0.7392 - val_accuracy: 0.5791\n",
      "Epoch 25/100\n",
      "13593/13593 [==============================] - 34s 3ms/step - loss: 0.7233 - accuracy: 0.6030 - val_loss: 0.7400 - val_accuracy: 0.5771\n",
      "Epoch 26/100\n",
      "13593/13593 [==============================] - 38s 3ms/step - loss: 0.7227 - accuracy: 0.6051 - val_loss: 0.7368 - val_accuracy: 0.5936\n",
      "Epoch 27/100\n",
      "13593/13593 [==============================] - 38s 3ms/step - loss: 0.7217 - accuracy: 0.6058 - val_loss: 0.7456 - val_accuracy: 0.5897\n",
      "Epoch 28/100\n",
      "13593/13593 [==============================] - 34s 2ms/step - loss: 0.7225 - accuracy: 0.6017 - val_loss: 0.7390 - val_accuracy: 0.5877\n",
      "Epoch 29/100\n",
      "13593/13593 [==============================] - 33s 2ms/step - loss: 0.7197 - accuracy: 0.6054 - val_loss: 0.7445 - val_accuracy: 0.5804\n",
      "Epoch 30/100\n",
      "13593/13593 [==============================] - 34s 3ms/step - loss: 0.7201 - accuracy: 0.6064 - val_loss: 0.7456 - val_accuracy: 0.5811\n",
      "Epoch 31/100\n",
      "13593/13593 [==============================] - 35s 3ms/step - loss: 0.7190 - accuracy: 0.6060 - val_loss: 0.7429 - val_accuracy: 0.5831\n",
      "Epoch 32/100\n",
      "13593/13593 [==============================] - 33s 2ms/step - loss: 0.7181 - accuracy: 0.6097 - val_loss: 0.7448 - val_accuracy: 0.5884\n",
      "Epoch 33/100\n",
      "13593/13593 [==============================] - 34s 3ms/step - loss: 0.7168 - accuracy: 0.6051 - val_loss: 0.7470 - val_accuracy: 0.5943\n",
      "Epoch 34/100\n",
      "13593/13593 [==============================] - 35s 3ms/step - loss: 0.7150 - accuracy: 0.6119 - val_loss: 0.7459 - val_accuracy: 0.5758\n",
      "Epoch 35/100\n",
      "13593/13593 [==============================] - 33s 2ms/step - loss: 0.7137 - accuracy: 0.6145 - val_loss: 0.7512 - val_accuracy: 0.5831\n",
      "Epoch 36/100\n",
      "13593/13593 [==============================] - 34s 3ms/step - loss: 0.7126 - accuracy: 0.6110 - val_loss: 0.7471 - val_accuracy: 0.5797\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00036: early stopping\n",
      "808\n",
      "20190117 accuracy:  0.5878712871287128\n",
      "5342\n",
      "20190118 accuracy:  0.6198053163609135\n",
      "2570\n",
      "20190122 accuracy:  0.6070038910505836\n",
      "2944\n",
      "20190123 accuracy:  0.6321331521739131\n",
      "1759\n",
      "20190124 accuracy:  0.5992040932347925\n",
      "1610\n",
      "20190125 accuracy:  0.5739130434782609\n",
      "1428\n",
      "20190128 accuracy:  0.5749299719887955\n",
      "1034\n",
      "20190129 accuracy:  0.5841392649903289\n",
      "2164\n",
      "20190130 accuracy:  0.5679297597042514\n",
      "2679\n",
      "20190131 accuracy:  0.606942889137738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<models.LSTM_Model at 0x10e955890>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model = LSTM_Model(time_steps = time_steps,  hidden_dim = hidden_dim, n_epochs = n_epochs,\n",
    "                        activation = activation, loss = loss, \n",
    "                        data_dim = pipeline.x.shape[-1], output_dim = pipeline.y.shape[-1])\n",
    "lstm_model.build()\n",
    "pipeline.model_training_testing(ticker, lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15154, 50, 4) (15154, 3)\n"
     ]
    }
   ],
   "source": [
    "print(pipeline.x.shape, pipeline.y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Conv2D, Flatten, Dropout\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 30\n",
    "n_epochs = 100\n",
    "batch_size = 128\n",
    "activation = \"relu\"\n",
    "loss = 'categorical_crossentropy'\n",
    "stop_patience=20\n",
    "\n",
    "dpi=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n",
      "train (13593, 50, 4, 1) (13593, 3)\n",
      "validation (1511, 50, 4, 1) (1511, 3)\n",
      "Train on 13593 samples, validate on 1511 samples\n",
      "Epoch 1/100\n",
      "13593/13593 [==============================] - 5s 386us/step - loss: 0.8514 - accuracy: 0.4911 - val_loss: 0.8225 - val_accuracy: 0.4811\n",
      "Epoch 2/100\n",
      "13593/13593 [==============================] - 3s 228us/step - loss: 0.7923 - accuracy: 0.5602 - val_loss: 0.7632 - val_accuracy: 0.5831\n",
      "Epoch 3/100\n",
      "13593/13593 [==============================] - 3s 222us/step - loss: 0.7708 - accuracy: 0.5791 - val_loss: 0.7417 - val_accuracy: 0.5970\n",
      "Epoch 4/100\n",
      "13593/13593 [==============================] - 3s 230us/step - loss: 0.7587 - accuracy: 0.5857 - val_loss: 0.7356 - val_accuracy: 0.6009\n",
      "Epoch 5/100\n",
      "13593/13593 [==============================] - 3s 244us/step - loss: 0.7525 - accuracy: 0.5904 - val_loss: 0.7574 - val_accuracy: 0.5817\n",
      "Epoch 6/100\n",
      "13593/13593 [==============================] - 3s 250us/step - loss: 0.7502 - accuracy: 0.5922 - val_loss: 0.7379 - val_accuracy: 0.5910\n",
      "Epoch 7/100\n",
      "13593/13593 [==============================] - 3s 257us/step - loss: 0.7477 - accuracy: 0.5969 - val_loss: 0.7337 - val_accuracy: 0.5970\n",
      "Epoch 8/100\n",
      "13593/13593 [==============================] - 4s 261us/step - loss: 0.7444 - accuracy: 0.6008 - val_loss: 0.7335 - val_accuracy: 0.5917\n",
      "Epoch 9/100\n",
      "13593/13593 [==============================] - 4s 258us/step - loss: 0.7411 - accuracy: 0.5993 - val_loss: 0.7309 - val_accuracy: 0.5956\n",
      "Epoch 10/100\n",
      "13593/13593 [==============================] - 4s 264us/step - loss: 0.7401 - accuracy: 0.6008 - val_loss: 0.7327 - val_accuracy: 0.5903\n",
      "Epoch 11/100\n",
      "13593/13593 [==============================] - 3s 257us/step - loss: 0.7383 - accuracy: 0.6035 - val_loss: 0.7301 - val_accuracy: 0.5989\n",
      "Epoch 12/100\n",
      "13593/13593 [==============================] - 3s 238us/step - loss: 0.7364 - accuracy: 0.6066 - val_loss: 0.7299 - val_accuracy: 0.6069\n",
      "Epoch 13/100\n",
      "13593/13593 [==============================] - 3s 225us/step - loss: 0.7352 - accuracy: 0.6073 - val_loss: 0.7332 - val_accuracy: 0.5811\n",
      "Epoch 14/100\n",
      "13593/13593 [==============================] - 3s 220us/step - loss: 0.7343 - accuracy: 0.6073 - val_loss: 0.7366 - val_accuracy: 0.5811\n",
      "Epoch 15/100\n",
      "13593/13593 [==============================] - 3s 221us/step - loss: 0.7314 - accuracy: 0.6080 - val_loss: 0.7407 - val_accuracy: 0.5890\n",
      "Epoch 16/100\n",
      "13593/13593 [==============================] - 3s 223us/step - loss: 0.7312 - accuracy: 0.6103 - val_loss: 0.7338 - val_accuracy: 0.5884\n",
      "Epoch 17/100\n",
      "13593/13593 [==============================] - 3s 223us/step - loss: 0.7316 - accuracy: 0.6105 - val_loss: 0.7372 - val_accuracy: 0.5844\n",
      "Epoch 18/100\n",
      "13593/13593 [==============================] - 3s 220us/step - loss: 0.7314 - accuracy: 0.6117 - val_loss: 0.7407 - val_accuracy: 0.5817\n",
      "Epoch 19/100\n",
      "13593/13593 [==============================] - 3s 220us/step - loss: 0.7284 - accuracy: 0.6116 - val_loss: 0.7420 - val_accuracy: 0.5844\n",
      "Epoch 20/100\n",
      "13593/13593 [==============================] - 3s 220us/step - loss: 0.7275 - accuracy: 0.6166 - val_loss: 0.7394 - val_accuracy: 0.5771\n",
      "Epoch 21/100\n",
      "13593/13593 [==============================] - 3s 220us/step - loss: 0.7273 - accuracy: 0.6172 - val_loss: 0.7400 - val_accuracy: 0.5738\n",
      "Epoch 22/100\n",
      "13593/13593 [==============================] - 3s 224us/step - loss: 0.7257 - accuracy: 0.6164 - val_loss: 0.7442 - val_accuracy: 0.5831\n",
      "Epoch 23/100\n",
      "13593/13593 [==============================] - 3s 232us/step - loss: 0.7258 - accuracy: 0.6189 - val_loss: 0.7384 - val_accuracy: 0.5811\n",
      "Epoch 24/100\n",
      "13593/13593 [==============================] - 3s 232us/step - loss: 0.7258 - accuracy: 0.6154 - val_loss: 0.7394 - val_accuracy: 0.5950\n",
      "Epoch 25/100\n",
      "13593/13593 [==============================] - 3s 233us/step - loss: 0.7241 - accuracy: 0.6180 - val_loss: 0.7442 - val_accuracy: 0.5784\n",
      "Epoch 26/100\n",
      "13593/13593 [==============================] - 3s 241us/step - loss: 0.7209 - accuracy: 0.6239 - val_loss: 0.7436 - val_accuracy: 0.5758\n",
      "Epoch 27/100\n",
      "13593/13593 [==============================] - 3s 239us/step - loss: 0.7213 - accuracy: 0.6232 - val_loss: 0.7518 - val_accuracy: 0.5804\n",
      "Epoch 28/100\n",
      "13593/13593 [==============================] - 3s 239us/step - loss: 0.7220 - accuracy: 0.6243 - val_loss: 0.7427 - val_accuracy: 0.5870\n",
      "Epoch 29/100\n",
      "13593/13593 [==============================] - 3s 239us/step - loss: 0.7191 - accuracy: 0.6260 - val_loss: 0.7442 - val_accuracy: 0.5870\n",
      "Epoch 30/100\n",
      "13593/13593 [==============================] - 3s 229us/step - loss: 0.7181 - accuracy: 0.6249 - val_loss: 0.7427 - val_accuracy: 0.5771\n",
      "Epoch 31/100\n",
      "13593/13593 [==============================] - 3s 225us/step - loss: 0.7165 - accuracy: 0.6248 - val_loss: 0.7529 - val_accuracy: 0.5731\n",
      "Epoch 32/100\n",
      "13593/13593 [==============================] - 3s 219us/step - loss: 0.7177 - accuracy: 0.6272 - val_loss: 0.7516 - val_accuracy: 0.5659\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00032: early stopping\n",
      "808\n",
      "20190117 accuracy:  0.5693069306930693\n",
      "5342\n",
      "20190118 accuracy:  0.6177461624859603\n",
      "2570\n",
      "20190122 accuracy:  0.6066147859922179\n",
      "2944\n",
      "20190123 accuracy:  0.6039402173913043\n",
      "1759\n",
      "20190124 accuracy:  0.5741898806139852\n",
      "1610\n",
      "20190125 accuracy:  0.5857142857142857\n",
      "1428\n",
      "20190128 accuracy:  0.5812324929971989\n",
      "1034\n",
      "20190129 accuracy:  0.5986460348162476\n",
      "2164\n",
      "20190130 accuracy:  0.5651571164510166\n",
      "2679\n",
      "20190131 accuracy:  0.6047032474804032\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<models.CNN_Model at 0x1a5502af50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model = CNN_Model(time_steps = time_steps, hidden_dim = hidden_dim, n_epochs=n_epochs,\n",
    "                      activation = activation, loss = loss,\n",
    "                      data_dim = pipeline.x.shape[-1], output_dim = pipeline.y.shape[-1])\n",
    "cnn_model.build()\n",
    "pipeline.model_training_testing(ticker, cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth=36\n",
    "n_estimators=100\n",
    "max_features=0.2\n",
    "criterion = 'gini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n",
      "train (13593, 200) (13593, 3)\n",
      "validation (1511, 200) (1511, 3)\n",
      "808\n",
      "20190117 accuracy:  0.5693069306930693\n",
      "5342\n",
      "20190118 accuracy:  0.598090602770498\n",
      "2570\n",
      "20190122 accuracy:  0.5649805447470817\n",
      "2944\n",
      "20190123 accuracy:  0.5356657608695652\n",
      "1759\n",
      "20190124 accuracy:  0.5247299602046618\n",
      "1610\n",
      "20190125 accuracy:  0.5111801242236025\n",
      "1428\n",
      "20190128 accuracy:  0.5483193277310925\n",
      "1034\n",
      "20190129 accuracy:  0.5154738878143134\n",
      "2164\n",
      "20190130 accuracy:  0.5619223659889094\n",
      "2679\n",
      "20190131 accuracy:  0.5528182157521463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<models.RandomForest at 0x1a55c227d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_model = RandomForest(max_depth, n_estimators, max_features, criterion)\n",
    "pipeline.model_training_testing(ticker, random_forest_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators=100\n",
    "max_depth=50\n",
    "learning_rate=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n",
      "train (13593, 200) (13593, 1)\n",
      "validation (1511, 200) (1511, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.7/site-packages/sklearn/ensemble/_gb.py:1454: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808\n",
      "20190117 accuracy:  0.46885875159298107\n",
      "5342\n",
      "20190118 accuracy:  0.4415097555577391\n",
      "2570\n",
      "20190122 accuracy:  0.4571628639343518\n",
      "2944\n",
      "20190123 accuracy:  0.4600288953582821\n",
      "1759\n",
      "20190124 accuracy:  0.4674854989252059\n",
      "1610\n",
      "20190125 accuracy:  0.47241773079742294\n",
      "1428\n",
      "20190128 accuracy:  0.47131205423345807\n",
      "1034\n",
      "20190129 accuracy:  0.4763607929993378\n",
      "2164\n",
      "20190130 accuracy:  0.4702111684735258\n",
      "2679\n",
      "20190131 accuracy:  0.472135800812619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<models.GradientBoost at 0x1a46213650>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_boost_model=GradientBoost(n_estimators=n_estimators,max_depth=max_depth,learning_rate=learning_rate)\n",
    "pipeline.model_training_testing(ticker, gradient_boost_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators=100\n",
    "max_depth=50\n",
    "learning_rate=0.1\n",
    "reg_lambda=0.1\n",
    "verbose=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n",
      "train (13593, 200) (13593, 1)\n",
      "validation (1511, 200) (1511, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/xunyingluo/anaconda3/envs/py36/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808\n",
      "20190117 accuracy:  0.47589390746005295\n",
      "5342\n",
      "20190118 accuracy:  0.44020008575544334\n",
      "2570\n",
      "20190122 accuracy:  0.45902481491014246\n",
      "2944\n",
      "20190123 accuracy:  0.45376915742409024\n",
      "1759\n",
      "20190124 accuracy:  0.4677983543417254\n",
      "1610\n",
      "20190125 accuracy:  0.47999382739863433\n",
      "1428\n",
      "20190128 accuracy:  0.475260692512299\n",
      "1034\n",
      "20190129 accuracy:  0.4854034397225475\n",
      "2164\n",
      "20190130 accuracy:  0.47351339854654045\n",
      "2679\n",
      "20190131 accuracy:  0.4738192243850913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<models.XGBoost at 0x1a452aad90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost_model=XGBoost(n_estimators=n_estimators, max_depth=max_depth,learning_rate=learning_rate,reg_lambda=reg_lambda, verbose=verbose)\n",
    "pipeline.model_training_testing(ticker, xgboost_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Tick Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TICK FACTOR\n",
    "# only update if it's a trade\n",
    "# if message_type == 't':\n",
    "#     # calc the tick\n",
    "#     this_tick = np.sign(last_price - prev_price)\n",
    "#     if this_tick == 0:\n",
    "#         this_tick = prev_tick\n",
    "\n",
    "#     # now calc the tick\n",
    "#     if tick_factor == 0:\n",
    "#         tick_factor = this_tick\n",
    "#     else:\n",
    "#         tick_factor = (tick_ema_alpha * this_tick) + (1 - tick_ema_alpha) * tick_factor\n",
    "\n",
    "#         # store the last tick\n",
    "#     prev_tick = this_tick\n",
    "\n",
    "for test_date in test_date_list:\n",
    "    df = pd.read_csv(data_dir+'trades_{}_{}.csv'.format(test_date, ticker),index_col=[0],parse_dates=[0])\n",
    "    \n",
    "    df[\"tick_test\"]=(df[\"trade_px\"]-df[\"trade_px\"].shift(1)).apply(lambda x: 1 if x>0. else (-1. if x<0 else np.nan))\n",
    "    df.fillna(method=\"ffill\",inplace=True)\n",
    "    \n",
    "    df[\"tick_factor\"]=df[\"tick_test\"].ewm(span=20).mean()\n",
    "    df.dropna(axis=0,inplace=True)\n",
    "    \n",
    "    mysign = lambda x: 0 if abs(x)<1e-5 else (1 if x>0 else -1)\n",
    "    df[\"predict\"]=df[\"tick_factor\"].apply(mysign)\n",
    "    df[\"real_movement\"]=(df[\"trade_px\"].rolling(nforward).mean().shift(-nforward)-df[\"trade_px\"]).apply(mysign)\n",
    "    \n",
    "    df.dropna(axis=0,inplace=True)\n",
    "    acc=np.mean(df[\"predict\"]==df[\"real_movement\"])\n",
    "    print(\"Accuracy of {}\".format(test_date),acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum=pd.Timedelta(0)\n",
    "count=0\n",
    "for test_date in test_date_list:\n",
    "    df = pd.read_csv(data_dir+'trades_{}_{}.csv'.format(test_date, ticker),index_col=[0],parse_dates=[0])\n",
    "    timestamp=pd.DataFrame({\"trade_time\":df.index})\n",
    "    dt=timestamp[\"trade_time\"].shift(-nforward)-timestamp[\"trade_time\"]\n",
    "    dt.dropna(axis=0,inplace=True)\n",
    "    dt.apply(lambda x:x.seconds).hist()\n",
    "    sum+=dt.sum()\n",
    "    count+=dt.shape[0]\n",
    "print(\"Average time interval between {} trades is\".format(nforward),sum/count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
